{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a2cac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 15:13:12.985317: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743941593.000276   19461 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743941593.004576   19461 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from mypackage.elastic import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from mypackage.sentence import SentenceChain, doc_to_sentences, iterative_merge\n",
    "from mypackage.clustering import chain_clustering, visualize_clustering, group_chains_by_label\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "doc = Document.from_json(\"../cached_docs/doc_0001.json\", text_path=\"article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef4138f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = doc_to_sentences(doc, model)\n",
    "merged = iterative_merge(sentences, threshold=0.6, round_limit=None, pooling_method=\"average\")\n",
    "\n",
    "labels, clusters = chain_clustering(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439d8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.backend import BaseEmbedder\n",
    "from bertopic.backend._utils import select_backend\n",
    "from bertopic.cluster import BaseCluster\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937bd454",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_embedding_model = BaseEmbedder()\n",
    "empty_dimensionality_model = BaseDimensionalityReduction()\n",
    "empty_cluster_model = BaseCluster()\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75bde7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model= BERTopic(\n",
    "        embedding_model=empty_embedding_model,\n",
    "        umap_model=empty_dimensionality_model,\n",
    "        hdbscan_model=empty_cluster_model,\n",
    "        ctfidf_model=ctfidf_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5493185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  7  7  7  7  7  7  7  7  7  7  7  7  3  3  3  3  3  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  8  8  8  8  8  8  8  8  8  8 10 10\n",
      " 10 10 10 10 10 10 10  4  4  4  4  4  4  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "dista = 1\n",
    "\n",
    "if dista == 0:\n",
    "    docs = [\"\\n\\n\".join([chain.text for chain in cluster]) for label, cluster in clusters.items() if label >= 0]\n",
    "    labels = list(filter(lambda x: x >= 0, clusters.keys()))\n",
    "\n",
    "elif dista == 1:\n",
    "    docs = list(chain.from_iterable([[chain.text, label] for chain in cluster] for label, cluster in clusters.items() if label >= 0))\n",
    "    docs = np.array(docs)\n",
    "    labels = docs[:, 1].astype(int)\n",
    "    docs = docs[:, 0]\n",
    "    print(labels)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs, y=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb695987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qc', 'gr', 'no', '2013', 'et', 'al', 'arxiv', '0264', '9381', '1088']\n",
      "['sgwb', 'gravitational', 'waves', 'ptas', 'xcite', 'also', 'has', 'sources', 'anisotropy', 'of']\n",
      "['phys', 'allen', 'takahashi', 'soda', 'ottewill', 'zaldarriaga', 'kuroyanagi', 'jackiw', 'hellings', 'mingarelli']\n",
      "['cg', 'mode', 'dominated', 'xmath218', 'xmath3', 'region', 'xmath2', 'by', 'when', 'applies']\n",
      "['pulsars', 'pulsar', 'signals', 'optimal', 'to', 'time', 'filter', 'correlated', 'xmath1', 'two']\n",
      "['parameters', 'b_', 'stokes', 'ast', 'times', 'with', 'parameter', 'harmonics', 'spin', 'electromagnetic']\n",
      "['1103', 'physrevd', '10', 'ph', 'doi', 'astro', 'arxiv', 'physrevlett', '85', 'phys']\n",
      "['gv', 'orfs', 'fig', 'curve', 'generalized', 'red', 'orf', 'for', 'dipole', 'find']\n",
      "['xi', 'right', 'left', 'cos', 'gamma', 'frac', '34', 'gamma_', '11', '12']\n",
      "['curve', 'shows', 'gi', 'dashed', 'blue', 'dark', 'green', 'xmath266', 'dash', 'xmath267']\n",
      "['average', 'ensemble', 'xmath65', 'bracket', 'temporal', 'over', 'here', 'represents', 'amplitudes', 'fourier']\n",
      "{0: 'cluster_01', 1: 'cluster_06', 2: 'cluster_02', 3: 'cluster_09', 4: 'cluster_05', 5: 'cluster_07', 6: 'cluster_00', 7: 'cluster_08', 8: 'cluster_10', 9: 'cluster_04', 10: 'cluster_03'}\n"
     ]
    }
   ],
   "source": [
    "from itertools import starmap, groupby\n",
    "\n",
    "info_df = topic_model.get_topic_info()\n",
    "\n",
    "for row in info_df['Representation']:\n",
    "    print(row)\n",
    "\n",
    "newdic = {\n",
    "            topic: f\"cluster_{cluster:02}\"\n",
    "            for cluster, topic in sorted(topic_model.topic_mapper_.get_mappings().items(), key=lambda item: item[1])\n",
    "}\n",
    "\n",
    "print(newdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04fb6541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see where each chain belongs\n",
    "\n",
    "docs = list(chain.from_iterable([[chain.text, label] for chain in cluster] for label, cluster in clusters.items() if label >= 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aab6b974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0], [1.0])\n"
     ]
    }
   ],
   "source": [
    "topic_model.embedding_model = select_backend(model)\n",
    "print(topic_model.find_topics(search_term=\"what are sunspots\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
