{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"Test\"\n",
    "sentences = []\n",
    "\n",
    "def on_value_change(change):\n",
    "    global mode\n",
    "    mode = change['new']\n",
    "\n",
    "radio = widgets.RadioButtons(\n",
    "    options=['Test', 'Query', 'Input'],\n",
    "    value='Test',\n",
    "    description=\"Select:\"\n",
    ")\n",
    "radio.observe(on_value_change, names=\"value\")\n",
    "display(radio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"Test\":\n",
    "    sentences = [\n",
    "        \"Given an input text, it outputs a vector which captures the semantic information\",\n",
    "        \"By default, input text longer than 256 word pieces is truncated.\"\n",
    "    ]\n",
    "elif mode == \"Query\":\n",
    "    \n",
    "    from elastic import parse_queries, query, elasticsearch_client\n",
    "    import metrics\n",
    "\n",
    "    q = input(\"Query number: \")\n",
    "    if q == \"\":\n",
    "        q = 0\n",
    "    else: q = int(q)\n",
    "\n",
    "    query_list = parse_queries(\"../collection\")[q:q+1]\n",
    "    docs, multiple_query_results = query(elasticsearch_client(\"../credentials.json\", \"../http_ca.crt\"), query_list)\n",
    "    sentences = docs[0][0].split(\". \")[:-1]\n",
    "    print(f\"{query_list[0].text}\")\n",
    "    print(f\"Returned doc {multiple_query_results[0][0]} with relevance {metrics.relevance(multiple_query_results[0][0], query_list[0])}\")\n",
    "elif mode == \"Input\":\n",
    "    doc = input(\"Give text: \")\n",
    "    sentences = doc.split(\". \")[:-1]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try docs:\n",
    "30, 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "    print(s.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vec1, vec2) -> float:\n",
    "    return np.dot(vec1, vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
    "\n",
    "series = []\n",
    "\n",
    "for vec1, vec2 in zip(embeddings[:-1], embeddings[1:]):\n",
    "    series.append(cosine_sim(vec1, vec2))\n",
    "\n",
    "print(\"\\n\".join(map(str, series)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series of sentence similarity\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "#ax.plot(series)\n",
    "ax.step(np.arange(len(series)), series, where='post')\n",
    "ax.set_xticks(np.arange(len(series)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    arr = list(map(abs, arr))\n",
    "    maxval = max(arr)\n",
    "    minval = min(arr)\n",
    "\n",
    "    return list(map(lambda elem: (elem - minval)/(maxval-minval), arr))\n",
    "\n",
    "dy = np.gradient(series, np.arange(len(series)))  # Compute numerical derivative\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "\n",
    "#ax2.plot(normalize(dy))\n",
    "ax2.step(np.arange(len(series)), normalize(dy),where='post')\n",
    "#ax2.step(np.arange(len(series)), dy, where='post')\n",
    "ax2.set_xticks(np.arange(len(series)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from kmedoids import KMedoids\n",
    "import hdbscan\n",
    "from kneed import KneeLocator\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "num_clusters = 2\n",
    "\n",
    "'''\n",
    "# Generate the linkage matrix\n",
    "Z = linkage(embeddings, method='ward')\n",
    "print(Z)\n",
    "labels = fcluster(Z, t=num_clusters, criterion='maxclust')\n",
    "\n",
    "#Organize each cluster elements into lists\n",
    "clusters = [[] for _ in range(num_clusters)]\n",
    "\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(f\"Cluster {i:02}: {cluster}\")\n",
    "\n",
    "#Evaluate clustering\n",
    "score = silhouette_score(embeddings, labels)\n",
    "print(f\"\\nSilhouette Score: {score:.3f}\")'\n",
    "'''\n",
    "\n",
    "'''\n",
    "dista = (1 - cosine_similarity(embeddings)).astype(np.float64)\n",
    "\n",
    "clustering = hdbscan.HDBSCAN(min_cluster_size=2, metric=\"precomputed\")\n",
    "model = clustering.fit(dista)\n",
    "print(model.labels_)\n",
    "\n",
    "clusters = [[] for _ in range(num_clusters)]\n",
    "\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(f\"Cluster {i:02}: {cluster}\")\n",
    "\n",
    "score = silhouette_score(embeddings, model.labels_)\n",
    "print(f\"\\nSilhouette Score: {score:.3f}\")'\n",
    "'''\n",
    "dista = cosine_distances(embeddings)\n",
    "\n",
    "inertia = []\n",
    "K_range = list(range(1, len(sentences)))\n",
    "\n",
    "#Find optimal cluster count\n",
    "for k in K_range:\n",
    "    clustering = KMedoids(n_clusters=k, metric=\"precomputed\")\n",
    "    clustering_model = clustering.fit(dista)\n",
    "    inertia.append(clustering_model.inertia_)\n",
    "    print(clustering_model.inertia_)\n",
    "\n",
    "knee_locator = KneeLocator(K_range, inertia, curve=\"convex\", direction=\"decreasing\")\n",
    "optimal_k = knee_locator.elbow\n",
    "optimal_k = 3\n",
    "print(optimal_k)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(K_range, inertia, \"ro--\")\n",
    "plt.show()\n",
    "\n",
    "#Cluster optimal\n",
    "clustering = KMedoids(n_clusters=int(optimal_k), metric=\"precomputed\")\n",
    "clustering_model = clustering.fit(dista)\n",
    "medoids = clustering_model.medoid_indices_\n",
    "print(f\"Clustering: {clustering_model.labels_}\")\n",
    "print(f\"Medoids: {medoids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another idea\n",
    "Let's compare the query with each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"Query\":\n",
    "    print(f\"Query: {query_list[0].text}\")\n",
    "    query_vec = model.encode(query_list[0].text)\n",
    "\n",
    "    embedding_norms = np.linalg.norm(embeddings, axis=1)\n",
    "    query_sim = np.dot(embeddings, query_vec)\n",
    "    print(query_sim)\n",
    "\n",
    "    sorted_sentences = sorted(sentences, key=lambda x: query_sim[sentences.index(x)], reverse=True)\n",
    "    print(sorted_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize clusters and query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=2)\n",
    "\n",
    "S = pca_model.fit_transform(embeddings)\n",
    "\n",
    "if mode == \"Query\":\n",
    "    query_vec_reduced = pca_model.transform(query_vec.reshape(1, -1))\n",
    "    print(query_vec_reduced)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "markers = [\"x\" if i in medoids else \"o\" for i in range(len(sentences))]\n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "for i, elem in enumerate(S):\n",
    "    ax.scatter(elem[0], elem[1], c=colors[clustering_model.labels_[i]], edgecolors='k', alpha=0.7, marker=markers[i])\n",
    "\n",
    "if mode == \"Query\":\n",
    "    ax.scatter(query_vec_reduced[0, 0], query_vec_reduced[0, 1], c=\"red\", edgecolors='k', alpha=0.7, marker=\"^\")\n",
    "\n",
    "print(clustering_model.labels_)\n",
    "\n",
    "print(silhouette_score(dista, clustering_model.labels_, metric=\"precomputed\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
